# 第八章

## 主成分分析

>PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。

1. 分别求每个特征的平均值，然后对于所有的样例，都减去对应的均值，即`样本中心化`。
1. 求样本的协方差矩阵（对角线上分别是两两特征之间的方差，非对角线上是协方差。协方差大于0表示两两特征之间若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。）
1. 求协方差的特征值和特征向量
1. 将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。
1. 将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为`DataAdjust(m*n)`，协方差矩阵是`n*n`，选取的k个特征向量组成的矩阵为`EigenVectors(n*k)`。那么投影后的数据FinalData为`FinalData(m*k) = DataAdjust(m*n) * EigenVectors(n*k)`。

`这样，就将原始样例的n维特征变成了k维，这k维就是原始特征在k维上的投影。`

*__总结：__*  
PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。  
PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。  
但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。  
*__PCA方法的优点:__*

1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响；
1. 各主成分之间正交，可消除原始数据成分间的相互影响的因素;
1. 计算方法简单，主要运算是特征值分解，易于实现。

*__PCA方法的缺点:__*

1. 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强；
1. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

## LDA vs PCA

*__相同点:__*

1. 两者均可以对数据进行降维；
1. 两者在降维时均使用了矩阵特征分解的思想；
1. 两者都假设数据符合高斯分布。

*__不同点:__*

1. LDA是有监督的降维方法，而PCA是无监督的降维方法；
1. LDA降维最多降到类别数C-1的维数，而PCA没有这个限制；
1. LDA除了可以用于降维，还可以用于分类；
1. LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。

## 多维尺度方法

1. 首先计算数据点两两之间的距离矩阵<img src="http://latex.codecogs.com/gif.latex?\inline&space;\Delta" title="\Delta" />；
1. 双中心化距离矩阵<img src="http://latex.codecogs.com/gif.latex?\inline&space;\Delta" title="\Delta" /> ，由公式<img src="http://latex.codecogs.com/gif.latex?\inline&space;b_{j,k}&space;=&space;-\frac{1}{2}&space;\Delta&space;_{j,k}^{2}" title="b_{j,k} = -\frac{1}{2} \Delta _{j,k}^{2}" />得到内积矩阵Ｂ；
1. 计算内积矩阵Ｂ特征值和特征向量；
1. 把内积矩阵Ｂ分解成YTY,从而得到低维坐标Y．