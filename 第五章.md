# 第五章

## 神经网络

## 支持向量机

>支持向量机（Support Vector Machine, SVM）的基本模型是在特征空间上找到最佳的分离超平面使得训练集上正负样本间隔最大。SVM是用来解决二分类问题的有监督学习算法，在引入了核方法之后SVM也可以用来解决非线性问题。

SVM基本思想：

1. 通过非线性变换将输入空间变换到一个高维空间。
1. 在这个新空间求解最优分类面即最大间隔分类面。
1. 非线性变换通过适当的内积核函数实现。
1. 求解一个有约束的二次优化问题，有唯一最优解，这与多层感知器网络相比是一个优势。
1. 问题的计算复杂度由样本数目决定，不取决与样本变换特征维数与所采用的核函数。

### SVM方法的优点：

1. 分类器的复杂度取决于支持向量的个数，而不是变换空间的维数。因此SVM不容易发生过拟合的现象。
1. 当原训练样本线性不可分时，可利用某个非线性变换将原数据空间中的样本点映射到更高维空间中，使得在此高维空间中的映射点线性可分。

### SVM方法的缺点：

(1) SVM算法对大规模训练样本难以实施
由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。针对以上问题的主要改进有有J.Platt的SMO算法、T.Joachims的SVM、C.J.C.Burges等的PCGC、张学工的CSVM以及O.L.Mangasarian等的SOR算法
(2) 用SVM解决多分类问题存在困难
经典的支持向量机算法只给出了二类分类的算法，而在数据挖掘的实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗集理论结合，形成一种优势互补的多类问题的组合分类器。

### 核函数与支持向量机：

>支持向量机并没有直接计算复杂的线性变化，采用了巧妙的迂回方法实现这种变化

1. 多项式核函数
2. 径向基核函数(RBF)
3. Sigmoid函数

## 广义线性判别函数：

1. 经过变换，维数大大增加。使得问题很快陷入“维数灾难”，计算变得非常复杂而不可行。
1. 样本变换到高维空间后样本数目并未增加，在高维空间变得很稀疏，算法会因为病态矩阵等问题无法进行。