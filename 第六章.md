# 近邻法

近邻法的缺陷：

* 需存储所有训练样本
* 新样本需与每个样本做比较

## 近邻法的快速方法

基本思想：

* 把样本集分成多个子集（树状结构）
* 每个子集（结点）可用较少几个量代表
* 通过将新样本与各节点比较排除大量候选样本
* 只有最后的节点（子集）中逐个样本比较，找出近邻

例如：分支定界算法

## 最近邻法

>对于一个新样本，把它逐一与已知样本比较，找出距离新样本最近的已知样本，以该样本的类别作为新样本的类别。

## K-近邻法

找出x的k个近邻，看其中多数属于哪一类，则把x分到哪一类。

## 罗杰斯特回归

线性回归:  
`y = f(x)`  
罗杰斯特函数:  
<img src="http://latex.codecogs.com/png.latex?\inline&space;P(y&space;=&space;1|x)&space;=&space;\frac{e^{f(x)}}{1&space;&plus;&space;e^{f(x)}}" title="P(y = 1|x) = \frac{e^{f(x)}}{1 + e^{f(x)}}" />

几率：  
<img src="http://latex.codecogs.com/png.latex?\inline&space;\frac{P(y|x)}{1&space;&plus;&space;P(y|x)}&space;=&space;e^{f(x)}" title="\frac{P(y|x)}{1 + P(y|x)} = e^{f(x)}" />

对数几率：  
<img src="http://latex.codecogs.com/png.latex?\inline&space;ln(\frac{P(y|x)}{1&space;&plus;&space;P(y|x)})&space;=&space;f(x)" title="ln(\frac{P(y|x)}{1 + P(y|x)}) = f(x)" />

罗杰斯特回归：  
<img src="http://latex.codecogs.com/png.latex?\inline&space;logit(x)&space;=&space;ln(\frac{P(y|x)}{1&space;&plus;&space;P(y|x)})" title="logit(x) = ln(\frac{P(y|x)}{1 + P(y|x)})" />

决策规则：  
若logit(x) > 0，则x属于1类，否则属于2类

## 决策树

决策树的一些优点是：  

1. 很容易理解和解释。树可以被可视化。
1. 需要很少的数据准备。其他技术通常需要数据标准化，需要创建虚拟变量并删除空值。但是决策树对于缺失值没法处理
1. 预测数据是用于训练树的数据点的数量的对数。
1. 能够处理数字和分类数据。其他技术通常用于分析只有一种类型变量的数据集。
1. 能够处理多输出问题。
1. 使用白盒模型。如果给定的情况在模型中是可观察的，则条件的解释很容易通过布尔逻辑来解释。相比之下，在黑盒模型（例如，在人工神经网络中），结果可能更难以解释。
1. 可以使用统计测试来验证模型。这可以说明模型的可靠性。

决策树的缺点包括：  

1. 决策树学习可能创建过于复杂的树，不能很好地概括数据。这称为过度拟合。剪枝（目前不支持）等机制，就是设置叶节点所需的最小样本数或设置树的最大深度是避免此问题所必须的。
1. 决策树可能不稳定，因为数据中的小变化可能会导致生成完全不同的树。通过在集合中使用决策树可以缓解这个问题。
1. 学习最优决策树的问题在最优化的几个方面甚至简单的概念下被认为是NP完全的。
1. 有些概念很难学到，因为决策树不能很容易表达它们，例如XOR，奇偶校验或多路复用器问题。
1. 如果某些类占主导地位，决策树学习者会创建偏向性树。因此，建议在拟合决策树之前平衡数据集。

### 信息熵

1. 熵（entropy）是表示随机变量不确定性的度量。
1. 两个随机变量X，Y的联合分布，可形成联合熵。
1. 条件熵H(X|Y) = H(X,Y) – H(Y)是X，Y的联合熵减去Y的熵，表示在Y发生的前提下，X发生“新”带来的信息熵
1. 相对熵，又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等，其可以度量两个随机变量的“距离”

### 信息增益

>信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度。

定义：特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：
g(D,A)=H(D) – H(D|A)
显然，这即为训练数据集D和特征A的互信息。

### ID3

>ID3决策树最终产生的是一颗类似于多叉树的数据结构。ID3算法的基础是香农信息论中定义的信息熵。该算法中每次选取导致熵不纯度最小的特征作为分类特征，产生一颗决策树。

### C4.5

C4.5算法是建立在ID3算法上的，多了对连续数值的处理和ID3算法信息增益的缺点（C4.5算法采用了信息增益率）。  
在 ID3 中，我们不能解决连续变量问题，ID3 的做法就是对每一个不同值的变量进行分开计算，这样就出现了一个问题，ID3 构建的决策树中产生了过多的分支。这个时候可以把这些值修改成某一个域值，让小于等于这个域值的数据放在决策树的左边，大于这个域值的数据放在决策树的右边。C4.5 中就是这么干的，只是 C4.5 在寻找这个域值时，更加合理。  
假设这样的情况，每个属性中每种类别都只有一个样本，那这样属性信息熵就等于零，根据信息增益就无法选择出有效分类特征。所以，C4.5选择使用信息增益率对ID3进行改进。

### CART

>核心思想与ID3和C4.5相同，但是它是一颗二叉树，每个节点都只有两个子节点。

### 分类器集成

单个分类器的训练总的来说由两大基本要素确定：训练样本和训练算法  

* 使用不同的训练样本
* 使用不同的训练算法（不同的参数也算）

### 选择性集成

>分类器集成通过利用多个分类器来获得比及使用单个分类器更强的泛化能力。

尽管如此，分类器也并非越多越好,因为  

* 更多的分类器导致更大的计算和存储开销
* 越多的分类器会使个体之间的差异越来越难获得
* 实验证实了过多分类器可能降低集成的泛化能力

选择性集成：使用部分训练好的分类器，从而在已有的个体分类器中进行选择之后再集成。

目标：获得更好的泛化能力，同时降低存储和计算开销。

### Bagging

>Bagging 主要是减少分类模型的方差(variance)。因此，Bagging 一般用于训练算法对于训练数据较为敏感(即unstable)的场合(例如：决策树)

1. 从样本集中重采样(有重复的)选出n个样本
1. 在所有属性上，对这n个样本建立分类器(ID3、C4.5、CART、SVM、Logistic回归等)
1. 重复以上两步m次，即获得了m个分类器
1. 将数据放在这m个分类器上，最后根据这m个分类器的投票结果，决定数据属于哪一类。

### 随机森林

>随机森林在bagging基础上做了修改。

1. 从样本集中用Bootstrap采样（重采样）选出n个样本；
1. 从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树；
1. 重复以上两步m次，即建立了m棵CART决策树
1. 这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类。

### Boosting

Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数系列,然后以一定的方式将他们组合成一个预测函数。他是一种框架算法,主要是通过对样本集的操作获得样本子集,然后用弱分类算法在样本子集上训练生成一系列的基分类器。他可以用来提高其他弱分类算法的识别率,也就是将其他的弱分类算法作为基分类算法放于Boosting 框架中,通过Boosting框架对训练样本集的操作,得到不同的训练样本子集,用该样本子集去训练生成基分类器;每得到一个样本集就用该基分类算法在该样本集上产生一个基分类器,这样在给定训练轮数 n 后,就可产生 n 个基分类器,然后Boosting框架算法将这 n个基分类器进行加权融合,产生一个最后的结果分类器,在这 n个基分类器中,每个单个的分类器的识别率不一定很高,但他们联合后的结果有很高的识别率,这样便提高了该弱分类算法的识别率。在产生单个的基分类器时可用相同的分类算法,也可用不同的分类算法,这些算法一般是不稳定的弱分类算法,如神经网络(BP) ,决策树(C4.5)等。

### AdaBoost

1. 基本思路：每一个样本都被赋予一个权重，表明其被选为训练集的概率修改权重的规则：如果某样本已被正确分类，则降低权重，否则提高权重。
1. 最后根据每个分量分类器的性能，以加权投票的方式进行决策。